{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 2em; font-weight:bold\">AI 70's Country</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import tensorflow\n",
    "#import keras\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense,LSTM,Dropout\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "import string\n",
    "\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, data prep is the hardest part of the project.  Because I am going to use a validation set in training my model, and because Keras uses the last n% of the data as the validation set, I want to shuffle the lyrics so that my validation set contains a better representation of all the data - not just the last song.  I also want to get the most originality that I can out of the model, so I will eliminate duplicate lyrics (some songs have refrains that repeat multiple times).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricsCSV = pd.read_csv('lyricsTrain_35.csv',encoding='ISO-8859-1')\n",
    "lyricsCSV.sort_values(lyricsCSV.columns[0],inplace=True)\n",
    "lyricsCSV.drop_duplicates(keep='first',inplace=True)\n",
    "lyricsCSV = lyricsCSV.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "lyricsCSV.to_csv('lyrics.txt',sep='\\t',index=False)\n",
    "l = open('lyrics.txt','r')\n",
    "lyrics = l.read()\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove lines and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics I've been walkin' these streets so long When the sun's comin' up I got cakes on the griddle If I said you had a beautiful body When I was lost you took me home I beg your pardon That's all I'm taking with me With flaming locks of auburn hair \"When Tommy turned around they said, \"\"hey look, old yeller's leaving\"\"\" Just like your daddy is A long time forgotten are dreams that just fell by the way It sure is cold today Tommy opened up the door, and saw his Becky crying Bad so I had one more for dessert Just when I'm about to make it work without you Kiss an angel good morning Even with someone they love Between Hank Williams' pain songs and Like your imitation love for me And did I hear you say he was a-meeting you here today Sleepin' in our king size bed \"Said, \"\"Live a good life and play the fiddle with pride\" Oh how real those roses seem to me My daddy taught me young how to hunt and how to whittle I'd smoked my brain the night before Would your flowing love come quench me \"\"\"Look like I'm gonna hafta haul you all in\" Your love for her grew From a mail order catalog People writing letters back home He turned around and grinned at me and said But what would it matter The secret I'm speaking of, is a woman & a man in love And here is what I'll say Make up her mind between And would you not be above me? Goodbye darlin' And it echoed thru the canyon like But there's one thing I want you to know Country roads, take me home At night we'd sleep 'cause we were tired And love her like the devil when you get back home Don't know when I've been so blue To the place I belong All at once you weren't here Find it in your heart They love our milk and honey \"I said \"\"Hey, judge, old buddy, old pal\"\"\" To take you to his mansion in the sky? Son, let me tell ya now exactly what I mean Anyplace is alright as long as I I should have held you but I let you go We had such a perfect year While she lays waiting, I stumble to the kitchen for a bite She had the world I'd be a fool 'cause I finally found someone who really cares Would you hold it against me Thank you oh Lord for making him for me Not much left but the floor, nothing lives here anymore In the park I saw a daddy And It's almost like a song You needed me, you needed me A lotta sad people thinkin' that's mighty keen Cries for you each night As you used to be Now tell me the truth If I should stay All the folks around Brownsville say she's crazy I'd pour me a drink, but I'd only be sorry That I've been pickin' Them that don't know him won't like him And you were a lady Sleeping single in a double bed With no way to hold my head Well, when he took us inta court I couldn't believe my eyes I will find a way Newberry's train songs and Blue Eyes Cryin' in the Rain All my memories gather round her And held me up and gave me dignity I never promised you a rose garden Say anything but don't say goodbye Somehow you needed me Would you miss your colored blouse If you don't love it, leave it Has been shattered by the closing of the door I'd have your baby Tossing, turning, trying to forget (ahh-ah) Out in Luckenbach, Texas there ain't nobody feelin' no pain But if you ever want somebody to just love ya, and some day you I'd play Sally Goodin all day if I could The judges wife called up \"But daddy always told me, \"\"Don't make small talk\"\"\" How's your new love Almost heaven, West Virginia About the way they have to live here in this country Or Phoenix Arizona Could it be a faded rose from days gone by? I sold my soul, you bought it back for me And I'm so sorry Don't let 'em pick guitars and drive them old trucks There's gotta be a little rain some time Do these old shoes look funny Than the last time I held you All you gotta do is smile that smile A raisin' me a family and workin' on a farm Saw a women walking I will find a way, find a way \"\"\"Ninety days, Jerry, when you hot, you hot\"\"\" My only prayer will be some day you'll care for me but it's only make believe I'm happiest girl, in the whole U.S.A You've got to kiss an angel good morning Honey, come back where you belong to only me. The work we done was hard It's early to rise, early in the sack If I were a miller When the party's all over she'll welcome him back home again Well life on the farm is kinda laid back One of them got up and met him halfway 'cross the floor While she lays dreaming, I try to get undressed without the light Got us feuding like the Hatfields and McCoys The destination was Well, I guess that's about all I gotta say. And you came to me Now please don't think I'm weak, I didn't turn the other cheek Along with the sunshine I cried a tear, you wiped it dry I'd not miss my colored blouse That I always thought it could be \"I said, \"\"Well, son when you hot, you hot\"\"\" My plans my hopes my schemes you are my every dream but it's only make believe On the sunday morning sidewalk While she lays sleeping, I stay out late at night and play my songs But it's much too sad to write What I'm tryin' to say is that Harpin' on the wars we fight And listened to the song Now I'm the one sleeping all alo-oh-one And a voice is softly saying My happiness depends on you And here I am'a walking down sixty-six Following behind you There goes my everything When I heard somethin' behind me \"\"\"I'll pay ya that hundred I owe ya if you'll get me outta this spot\"\"\" I don't want to be alone With each rising sun Would you still love me? Your smile is like a breath of spring Where hustle's the name of the game Runnin' down the way of life And it took me back to something I would only be in your way As much as I love waking up next to you You were in my arms While she lays, while she waits for me A doctor and a lawyer man His mama named him Tommy, but folks just called him yellow And when you're not, you're not Honey, come back I just can't stand Got a whole lot a good woman I remember well, the well where I drew water If a tinker was my trade I walk away from trouble when I can Except the memory of a coal miner's daughter Four car garage and we're still building on He was only ten years old when his daddy died in prison I guess he needs some time away from me Lisa left you years ago I'll make the bed And each step brings you closer Don't let 'em pick guitars or drive them old trucks Just one more minute In your high society you cry all day And driving down the road I get a feeling Mommy rocked the babies at night And ever' thing would start all over come break of morn And combed my hair And Jerry Jeff's train songs and Blue Eyes Cryin' in the Rain Big fine cars and fancy clothes. I wish you joy And shakin' me up so that all I really know One day while he was working, the Gatlin boys came calling He'll probably just ride away \"And quietly she says \"\"how was your night?\"\"\" As they gently walk across the lonely floor I'll be waiting for you This successful life we're livin' got us feudin' Let me kiss you His hand led hers away \"\"\"Promise me, son, not to do the things I've done\" Save your love through sorrow With ivory skin and eyes of emerald green I love you and I miss you This successful life we're livin' Bitter-sweet memories Gotta go, I love you There goes the one of my dreams Someone comes along I haven't slept a wink all night long When you're runnin' down our country, hoss \"\"\"You understand that, you hillbilly?\"\"\" Milo Venus was a beautiful lass Now the love that kept this old heart beating With my little songs, I was wrong Why'd we move that bojangle clock so far away from the bed Riding out on a horse in a star-spangled rodeo Sleeping single in a double bed (ooh-oo-ooh) If I were dying of thirst Of unemployment Mama don't let your babies grow up to be cowboys Just may, just give me a call-you know where I am Here you come again and here I go Thinking over things I wish I'd said But she has faith in me, and so I go on trying faithfully I could be lying with you instead \"\"\"When you hot, you hot\"\"\" But like a big red rose that's made of paper Another sleepless night and it's the same old stor-ory Back when ever since the world began \"n' I said \"\"Thanks a lot\"\"\" That's the one thing that daddy made sure of And I can easily understand Something always told me they were reading Tommy wrong And standin' up for things they believe in And kiss the happiest girl, in the whole U.S.A. Blue Ridge Mountains, Shenandoah River Singin' the same old song \"Sometimes you gotta fight when you're a man\"\"\" Good morning morning Lord knows she don't understand him, but she does the best that she can That I did you wrong From crying when he calls your name And your soft shoes shining? He reached above the fireplace, and took down his daddy's picture My eyes are not blue Well, my fiddle was my daddy's till the day he died Yonder comes a truck with the US mail They'd rather give you a song then diamonds or gold I got to go now \"\"\"I promised you, Dad, not to do the things you've done\" And I don't mind 'em switchin' sides But above all this People see us everywhere they all think you really care You make the coffee Little warm puppies and children and girls of the night And I can't believe it's you I'm the happiest girl, in the whole U.S.A. Put all the money in and let's roll 'em again Tossing, turning, trying to forget (you-ou) The kids are asleep so I keep it kinda low Is here you come again and here I go All I'm takin' is your time Younger than the mountains, blowing like a breeze But you don't know what he means to me You put me high upon a pedestal Go out and find herself Taught me how to love and how to give just a little You gave me hope when I was at the end Would you have my baby? I'll just cry all night long A good-hearted woman loving her good timing man They been walking thirty miles Between Hank Williams' pain songs You seemed so full of sweetness at the start Let me hold you in my arms Shine on me sunshine Three thousand years Now my broken heart Thank God I'm a country boy We were poor but we had love But his pride won't let him do things to make you think he's right Cause there's something in a sunday Ain't much an old country boy like me can't hack One more time While she lays crying, I fumble with a melody or two I won't try to understand In the palm of her hand I still recall the final words my brother said to Tommy I needed you and you were there But she lost both her arms And stumbled down the stairs I could promise you things like big diamond rings And pretty soon I'm wond'rin how I came to doubt you. And let her know you think about her when you're gone Everyone considered him the coward of the county I pull out my fiddle and I rosin up the bow The joy of love that used to taste like On the road to my horizon Darling this will be goodbye for evermore And I were a lady Now, it won't mean you're weak if you turn the other cheek It was almost like a song You waltz right in the door You could have your choice of men I'm begging of you please don't take my man And I cannot compete with you Once in every life I looked after Tommy, 'cause he was my brother's son It's a skippidity do da day Like the Hatfield and McCoys Is anybody goin' to San Antone Then I fumbled through my closet To meet the day You're messin' up my mind and fillin' up my senses Well people may try to guess, the secret of my happiness Is guitars that tune good and firm feelin' women Say it isn't true and Take the ribbon from my hair A man could wake up dead For my clothes I've been layin' in bed thinkin' like a woman Through teardrops and laughter, they'll pass through this world hand-in-hand, Cussin' at a can that he was kickin Why I've seen her fingers bleed (La la la la la la la) (La la la la la) Out in Luckenbach, Texas ain't nobody feelin' no pain In a cabin, on a hill in Butcher Holler Where you belong to only me. Her smile told of no night Cross the sand But you're down when you're ridin' the train that's takin' the long way And I will always love you Just like you've done before She loves him in spite of his ways that she don't understand I told her someday if she was my girl, I could change the world He wasn't holding nothin' back, he let 'em have it all All day long in the field a hoin' corn But if that's what it takes to hold you \"You even called me \"\"friend\"\"\" And I cry all night 'til dawn Then a man of low degree stood by her side But they preach about some other way of livin' This successful life we're livin's got us feudin' Well there goes my only possession Well I got me a fine wife I got me an ole fiddle \"He said \"\"Hello, boys\"\" and then he gave us a grin 'n' said\" Well, I was borned a coal miner's daughter The torn dress, the shattered look was more than he could stand To lose these memories And I stopped beside the Sunday school And a mill wheel grinding But I'd rather fight the wind and rain And tomorrow's out of sight Cause we both know that I'm not Daddy loved and raised eight kids on a miner's pay I hear her voice, in the morning hour she calls me Your voice is soft like summer rain Flying cross the desert in a TWA Tell me you love me and don't let me cry And who knows maybe on some special night, if my song is right And I wonder just how long As the tears fell on his daddy's face, he heard these words again And it's good when I finally make it home, all alone Well I woke up Sunday morning Cause she's a good-hearted woman; she loves her good timin' man Tell me no secrets, tell me some lies Mamma told her daughter Jolene, Jolene I didn't mean to treat you bad He was sitting in the witness stand Thinking over things I wish I'd said (ooh-ooh) I'm doin' alright That they were singing One more hug would do But they're only imitation And there's nothing I can do to keep I was confused, you cleared my mind Good-bye, please don't cry Sleeping under a table at a road side park Thinking over things I wish I'd said (ahh-ah) Yesterday is dead and gone I'm sleeping single in a double bed Beautiful daughter couldn't And she says to wake her up when I am through To think he might be out with another woman When ever I chance to meet, some old friends on the street I'll be fine when you're gone She was your morning light And a smile can hide all the pain My hopes my dreams come true my one and only you I wish you love But I just can't let you go without telling you just how much I love you. I could sing you a tune and promise you the moon I-I'm sleeping single in a double bed (ooh-oo-ooh) I've always got a smiling face, anytime & any place He'd never stood one single time to prove the county wrong My days are all filled with an easy country charm That makes a body feel alone I hear people talkin' bad, The rest of us can count on bein' free Hope you're doin' fine And I dream of the things I'll do So if I said you had a beautiful body He's the only one for me Honey, I know I've said it too many times before On the sleeping city sidewalk And I'll never leave, why should I leave? You're walkin' on the fightin' side of me I'm not ever gonna worry about tomorrow In a wrestling match to get When Tommy left the barroom, not a Gatlin boy was standing I'd answer you yes I would And nice guys get washed away like the snow and the rain A lonely bell was ringing I could by lying with you instead Then the flame became a dying ember My only prayer will be that some day you'll care for me but it's only make believe Right where you belong Let 'em be doctors and lawyers and such The rain can fall so soft against the window Back ever since the world began If you want your job you'd better But myself I can't deceive I know it's only make believe Just when I've begun to get myself together Paper roses, paper roses, Twenty years of crawling was bottled up inside him Wonderin' if my man's been doin' me wrong \"(When you're hot, you're hot\"\")\" If I were a carpenter You gave me strength to stand alone again With tender looks that I mistook for love I'd just as soon let you go \"\"\"Son, my life is over, but yours has just begun\"\"\" \"\"\"And I'd try a little bit of your honor on\"\"\" Hello darlin' \"Son, you don't have to fight to be a man\"\"\" Please don't take him even though you can This coat and tie is choking me And offers comin' over the phone And sometimes all the nights can be so long And so I'll go, but I know Just leave it up to you and in a little while Forget I've ever known her Where you've stayed for years And the answer is in this song that I always sing So baby, let's sell your diamond ring But I'm gonna be where the lights are shinin' on me Honey, come back And I kept rollin' them sevens , winnin' all them pots \"\"\"Judge\"\"\" But in the wintertime we'd all get a brand new pair January through December The disappearing dreams of yesterday Well a simple kinda life never did me no harm I could ask a lot of crazy questions And it's so good to be back home again I'll fix your lunch How it would feel to say No one will ever know just how much I love you so And every time they ask me why I just smile & say And send the kind that you remind me of I'd be carrying the pots you made With a subway token and a dollar tucked inside my shoe And there go all my defenses \"He said \"\"Well, when you're hot, you're hot\"\"\" I realise the way your eyes deceive me That didn't hurt I told her someday, if she was my girl, I could change the world Or let go oh-whoa-whoa-whoa Cause when he loves me he loves me a-all the way You're hot Skippidity do da Like the shadows on the wall Looking for a mysterious dark-haired man And turned my lies back into truth again I say, yeah, when you're hot, you're hot Cause drinking doubles alone, don't make it a par-arty I know those bright lights are callin' ya, honey. Taught me how to work and play a tune on the fiddle It's been a long time Nice to see you Well me and Homer Jones and Big John Talley Til the early mornin' light Thinking over things I wish I'd said (fade) Seems like a hundred years ago Come along and share the good times while we can Willie and Waylon and the boys Buy some boots and faded jeans and go away And here I go Yeah, city folk drivin' in a black limousine No one will ever know how much I love you so We've been so busy keepin' up with the Jones I never was one of them money hungry fools Walk away from trouble if you can And thank God you're a country boy Mamas' don't let your babies grow up to be cowboys Dark and dusty, painted on the sky To face the world out on my own again Who claims that he just don't believe in fightin' There's someone for everyone, and Tommy's love was Becky But when they're runnin' down our country, man You better look before you leap, still waters run deep Maybe it's time we got back to the basics of love My hopes my dreams come true my life I give for you Delta Dawn, what's that flower you have on? When you're the only one at two in the morning Runnin' down a way of life She's a good-hearted woman in love with a good-timin' man In the summertime we didn't have shoes to wear My heart I can't control you lure my very soul Are you happy? Look up darlin' My heart or wedding ring my all my everything Wish she hadn't done me that way Or I could ask what I really want to know I read about some squirrelly guy A brown eyede handsome man My one and only prayer is that some day you'll care They took turns at Becky, n'there was three of them And there's nothing short of dying When you're hot, you're hot What's that darlin' \"I said \"\"well I'll tell ya one thing judge, old buddy, old pal\"\"\" You held my hand when it was cold He like the night life, the bright lights and good-timin' friends Sleeping single in a double bed (ah-hah-ah) And if you should ever But you don't find roses growin' on stalks of clover Then I crossed the empty street and Of someone fryin chicken It won't mean you're weak if you turn the other cheek There goes my only possession Like a rhinestone cowboy And them that do sometimes won't know how to take him He talks about you in his sleep Well, now every time I rolled them dice I'd win Life is old there, older than the trees And read the Bible by the coal oil light \"He said \"\"Yeah\"\"\" The district attorney said you better Well I wouldn't trade my life for diamonds and jewels And you know what I'm talkin' about And he took me by the hand and held me close to his side And wrap my heart 'round your little finger \"\"\"If you wasn't wearin' that black robe I'd take out in back of this courthouse\" Our fightin' men have fought and died to keep In her younger days they called her Delta Dawn Arrested on charges And my soft shoes shining Getting cards and letters from people I don't even know I can't believe it's true I never thought of ever leaving Butcher Holler In her arms, he didn't have to prove he was a man I hope you're old enough to understand And somewhere far away So I'm just gonna take my bags and I'm gonna walk. Hello sunshine If you were a carpenter Take me home, country roads Sometimes when he comes home I'm cookin' breakfast When you take you gotta give so live and let live And promised her he'd take her for his bride And I wish you happiness Walk with me world He shoveled coal to make a poor man's dollar Comeback darlin' Help me make it through the night Love shouldn't be so melancholy Now shine on me sunshine In route to L.A. to get I would give you the world right now on a silver platter There once was a time when I could not imagine Just for old time's sake But all these years I've never caught him cheatin' My luck was so good I could do no wrong Except I can't sleep Daddy always managed to get the money somewhere I gave you my onlyness Free that brown eyed man She just talks about the good times they've had and all the good times to come And don't it make my brown eyes So I fiddle when I can, work when I should Then I headed back for home But you could've heard a pin drop when Tommy stopped and locked the door And found my cleanest dirty shirt Come and lay down by my side Then I see my old guitar in the night With a laughing little girl The only two things in life that make it worth livin' Now you be careful Don't it make my brown eyes blue Would you treat me like the devil tonight If you were a miller When he lo-oves me, he really lo-oves me As my memory turns back the pages Cause they'll never stay home and they're always alone When the work's all done and the sun's settlin' low Gettin' cards and letters from people I don't even know The radio reminds me of my home far away Would you marry me anyway And she believes in me, I'll never know just what she sees in me And I shaved my face That's what the trouble was If I swore you were an angel, Honey it's almost nine God, her love is true But some of them never learn it's a simple thing And there won't always be someone there to pull you out Tomorrow she'll probably want me back And thank you for letting life turn out the way But she never complains of the bad times or bad things he's done, Lord \"\"\"Pay for my Cadillac?\"\"\" Have a beautiful day, Don't know what's come over you Give me no reasons, give me alibies So take away the flowers that you gave me To complain, there was no need The sun can shine so bright up in the sky And I hope that you have all And the beer I had for breakfast wasn't Just waiting for me like a secret friend, and there's no end Had a big crap game goin' back in the alley I'd marry you anyway Is here you come again, and here I go, here I go How am I doin'? Shedding tears for Take me home, down country roads Wishing lord that I was stoned Cowboys like smokey old pool rooms and clear mountain mornin's May God bless you I said I'd never say it again They're walkin' on the fightin' side of me I know every crack in these dirty sidewalks of Broadway There'll be a load of compromisin' Here I go And papa, I sure hope you understand Mommy scrubbed our clothes on a washboard ever' day I turned around and there was a big old cop I can see the happy years we've had before I don't care what's right or wrong The good life he promised ain't what she's living today And who knows, maybe, on some special night, if my song is right And I love you will always love you It's too sad to write Half as lonesome as the sound Til the sunlight shines through your face That you ever dreamed of Thank you darlin' Would I still find you If you don't understand him and he don't die young Yeah, I'm proud to be a coal miner's daughter Like I aint got nothing on \"Whadda you mean 'contempt of court'?\"\"\" I'm sleeping single in a double bed (ah-hah-ah) I, I will always, always love you Money made from selling a hog She'd smile in mommy's understanding way Save your love through loneliness Misty taste of moonshine, teardrop in my eye But it sure seems warmer than it did My daddy worked all night in the Van Lear coal mines \"She's forty-one and her daddy still calls her \"\"baby\"\"\" But I could never love again Shake it loose and let it fall And I'm torn between the things that I should do Do you love waking up next to me He loved me all the way-ay Now we can talk all night about the weather Wind whipping down the neck of my shirt Well, I really don't mind the rain Jolene, Jolene, Jolene, Jolene But honey now I do I will always love you Let the devil take tomorrow That I'd lost somehow I got my song and I got you with me tonight But I'll still be just as gone Rain dripping off the brim of my hat Sunday morning coming down A brown eyed handsome man When you're runnin' down our country, man With Waylon and Willie and the boys \"And I come to her and say, \"\"it was all right, \"\" and I hold her tight\" You can 'splain it all down at City Hall Well a lot of things have changed since a way back then \"He said, \"\"Come on out and say what's on your mind\"\"\" I'd rather wonder a little and have his lovin' Rhinestone cowboy So he gave my friends a little fine to pay I guess I shouldn't say anything at all since you're supposed to belong to him. But the Lord and my wife wouldn't take it very good They wonder how does a man get to be this way Cowboys ain't easy to love and they're harder to hold \"I said \"\"Yeah?\"\"\" And you fix mine Til the sunlight has touched your face Don't it make my brown eyes Than what I've been fighting at home Caught the sunday smell So you better think it over But mine won't leave you And then one winter day To forgive me I hear footsteps slowly walking He let my friends go free and throwed the book at me \"\"\"Who gonna collect my welfare?\"\"\" So smile for a while and let's be jolly And I was just gettin' ready to roll 'em again And gripin' 'bout the way things oughta be (La la la la la la la, when you're hot, you're hot) She left you here drowning in your tears, here Here you come again What you need There's been a load of compromisin' Can forget I've ever known her But tonight I need a friend There goes my reason for living To the things you seek to find I hope life, treats you kind Lonestar belt buckles and old faded Levi's and each night begins a new day Lay it soft upon my skin And we were so in love So high that I could almost see eternity But when he lo-oves me, he really lo-oves me Here you come again lookin' better than a body has a right to I thought that you would be a perfect lover Each lonely day's a little bit longer Wake up sleepy head Oh there goes my everything Been a whole lot a good women If I worked my hands on wood Please don't take him just because you can Cause she walks dowtown with a suitcase in her hand I'll think of you each step of the way Life ain't nothin' but a funny funny riddle Makes my temper rise with jealousy Way back in history I got to try to find a way \"He said, \"\"this one's for Becky, as he watched the last one fall\" As long as he makes everything alright today-ay He ain't wrong he's just different And it's sad to be alone But I lit my first and watched a small kid Cause therels something in a sunday With cigarettes and songs Gettin' card and letters from people I don't even know Of a love so warm and true Just to know means so much to me Miner's lady, stranger to blue water Your beauty is beyond compare I-Im sleeping single in a double bed The Gatlin boys just laughed at him when he walked into the barroom The judge was a fishin' buddy that I recognized Let this song that I'm singin' be a warnin' And there got all my defenses Somewhere along the way West Virginia, mountain mama Didn't know just what I had You look into my eyes and lie those pretty lies I had to have this talk with you \"And keep all that money for evidence\"\"\" How you could easily take my man Tell you 'bout my friends out on the coast And whatever you decide to do I'm not Lisa, my name is Julie He was swingin So that is why I'm gonna say it one more time Back to his arms and never know Give me your tomorrow You've found someone new and Crying Lisa, Lisa That I should have been home yesterday, yesterday There's nothin' left for me to say-ay I don't need my name in the marquee lights Oh when he lo-oves me, he really lo-oves me Well if sweet-talkin' you could make it come true I jest kept on rollin' and controllin' them bones Prettiest woman you ever laid eyes on And finally they jest threw up their hands and said Jolene Let's go to Luckenbach, Texas You're just as lovely There isn't any sweetness in your heart She fought and won herself I'd rather have my fiddle and my farmin' tools That's why we moved it\n"
     ]
    }
   ],
   "source": [
    "tokens = lyrics.split()\n",
    "lyrics = ' '.join(tokens)\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing, when I look at the above lyrics, I seem to see a LOT of quotation marks.  So, I am going to just replace those with a space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = lyrics.replace('\"',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build sequences of characters that will be used to predict a final character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100 # Length of the characer sequences (because we have so much verbage,\n",
    "             # we can use a relatively large number)\n",
    "sequences = list()\n",
    "for i in range(length, len(lyrics)):\n",
    "    seq = lyrics[i-length:i+1]\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save a .txt file of our sequences with line endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '\\n'.join(sequences)\n",
    "file = open('char_sequences.txt','w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of character:number mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('char_sequences.txt','r')\n",
    "raw_text = file.read()\n",
    "file.close()\n",
    "\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Save the mapping as json \n",
    "json_map = json.dumps(mapping)\n",
    "__ = open('mapping.json','w')\n",
    "__.write(json_map)\n",
    "__.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dictionary to create sequences of numbers only (numbers that describe the characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in lines:\n",
    "    encoded_seq = [mapping[char] for char in line]\n",
    "    sequences.append(encoded_seq)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input sets (with 99 characters) and output sets (1 character) and then one-hot code the sets so we can use them to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X] #one-hot code input\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size) #one-hot code output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with tuning parameters determined by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def countryTrain(paramUnits,paramEpochs,paramValSplit,paramShuffle,paramBatchSize,paramDropout):\n",
    "\n",
    "    #Define callbacks\n",
    "    es = EarlyStopping(monitor = 'acc',min_delta = .01, patience = 2, mode = 'max',verbose=1)\n",
    "    mc = ModelCheckpoint('model.drop_{}.best'.format(str(paramDropout)), monitor='loss', mode='min', save_best_only=True) # Keep best model\n",
    "\n",
    "\n",
    "    # define and fit model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(paramUnits, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(paramDropout))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    modelLyrics = model.fit(X, y, epochs = paramEpochs, validation_split = paramValSplit, \n",
    "                            shuffle = paramShuffle, batch_size = paramBatchSize, verbose=1,callbacks=[es,mc])\n",
    "\n",
    "   \n",
    "    history = pd.DataFrame(modelLyrics.history)\n",
    "    history.to_csv('modelLyricsHistory_drop_{}.csv'.format(str(paramDropout)),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23347 samples, validate on 5837 samples\n",
      "Epoch 1/100\n",
      "23347/23347 [==============================] - 208s 9ms/sample - loss: 3.1541 - acc: 0.1972 - val_loss: 3.0216 - val_acc: 0.2042\n",
      "Epoch 2/100\n",
      "23347/23347 [==============================] - 206s 9ms/sample - loss: 2.9849 - acc: 0.2112 - val_loss: 2.7503 - val_acc: 0.2491\n",
      "Epoch 3/100\n",
      "23347/23347 [==============================] - 206s 9ms/sample - loss: 2.6797 - acc: 0.2662 - val_loss: 2.5323 - val_acc: 0.3073\n",
      "Epoch 4/100\n",
      "23347/23347 [==============================] - 206s 9ms/sample - loss: 2.5068 - acc: 0.3079 - val_loss: 2.4024 - val_acc: 0.3276\n",
      "Epoch 5/100\n",
      "23347/23347 [==============================] - 207s 9ms/sample - loss: 2.4104 - acc: 0.3328 - val_loss: 2.3228 - val_acc: 0.3342\n",
      "Epoch 6/100\n",
      "23347/23347 [==============================] - 206s 9ms/sample - loss: 2.3366 - acc: 0.3538 - val_loss: 2.2678 - val_acc: 0.3593\n",
      "Epoch 7/100\n",
      "19584/23347 [========================>.....] - ETA: 30s - loss: 2.2851 - acc: 0.3672"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e6b5e9c3bae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m modelLyrics = model.fit(X, y, epochs = epochs, validation_split = validationSplit, \n\u001b[1;32m---> 23\u001b[1;33m                             shuffle = shuffle, batch_size = batchSize, verbose=1,callbacks=[])\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mc:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "units = 100  # From the mentioned article in data science\n",
    "epochs = 100  # Just a large number since I am using early stopping\n",
    "validationSplit = 0.2 # My data set is small so I want to use as much as possible to train vs. validate\n",
    "shuffle = True\n",
    "batchSize = 64 # Doubled the default batch size to speed up training\n",
    "dropOut = .5  # http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "    #Define callbacks\n",
    "\n",
    "mc = ModelCheckpoint('model.100Epoch_6.6.best', monitor='acc', mode='max', save_best_only=True) # Keep best model\n",
    "\n",
    "\n",
    "    # define and fit model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, return_sequences=True,input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(units, return_sequences=True,input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(units, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(dropOut))\n",
    "model.add(Dense(vocab_size, activation='softmax',kernel_constraint=maxnorm(3)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "modelLyrics = model.fit(X, y, epochs = epochs, validation_split = validationSplit, \n",
    "                            shuffle = shuffle, batch_size = batchSize, verbose=1,callbacks=[])\n",
    "\n",
    "   \n",
    "#history = pd.DataFrame(modelLyrics.history)\n",
    "#history.to_csv('modelLyricsHistory.100Epoch_6.6.2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 2.9322 - acc: 0.2276 - val_loss: 2.5897 - val_acc: 0.3032\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.4171 - acc: 0.3365 - val_loss: 2.3031 - val_acc: 0.3582\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2339 - acc: 0.3676 - val_loss: 2.1955 - val_acc: 0.3913\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.1129 - acc: 0.4010 - val_loss: 2.1359 - val_acc: 0.3971\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.0526 - acc: 0.4241 - val_loss: 2.1109 - val_acc: 0.4191\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9236 - acc: 0.4550 - val_loss: 2.0108 - val_acc: 0.4513\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8273 - acc: 0.4849 - val_loss: 2.1109 - val_acc: 0.4486\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7193 - acc: 0.5059 - val_loss: 1.8773 - val_acc: 0.4665\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.5631 - acc: 0.5381 - val_loss: 1.8434 - val_acc: 0.4803\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.4330 - acc: 0.5733 - val_loss: 1.8295 - val_acc: 0.4839\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2985 - acc: 0.6101 - val_loss: 1.8329 - val_acc: 0.4884\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.1587 - acc: 0.6558 - val_loss: 1.8761 - val_acc: 0.4839\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.0136 - acc: 0.6941 - val_loss: 1.9417 - val_acc: 0.4861\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8742 - acc: 0.7400 - val_loss: 1.9659 - val_acc: 0.4888\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7420 - acc: 0.7793 - val_loss: 2.0353 - val_acc: 0.4924\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.6096 - acc: 0.8214 - val_loss: 2.1194 - val_acc: 0.4875\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4994 - acc: 0.8580 - val_loss: 2.2115 - val_acc: 0.4848\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4106 - acc: 0.8849 - val_loss: 2.2782 - val_acc: 0.4826\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3306 - acc: 0.9124 - val_loss: 2.4122 - val_acc: 0.4803\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2670 - acc: 0.9323 - val_loss: 2.4504 - val_acc: 0.4772\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2199 - acc: 0.9462 - val_loss: 2.5482 - val_acc: 0.4758\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1944 - acc: 0.9522 - val_loss: 2.5792 - val_acc: 0.4803\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1611 - acc: 0.9637 - val_loss: 2.6679 - val_acc: 0.4741\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1449 - acc: 0.9663 - val_loss: 2.7362 - val_acc: 0.4687\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1297 - acc: 0.9700 - val_loss: 2.7390 - val_acc: 0.4808\n",
      "Epoch 00025: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 77s 4ms/sample - loss: 2.9814 - acc: 0.2135 - val_loss: 2.6643 - val_acc: 0.2809\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.4664 - acc: 0.3241 - val_loss: 2.3350 - val_acc: 0.3381\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2813 - acc: 0.3631 - val_loss: 2.2876 - val_acc: 0.3775\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2158 - acc: 0.3930 - val_loss: 2.0950 - val_acc: 0.3998\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 2.0184 - acc: 0.4207 - val_loss: 2.0094 - val_acc: 0.4231\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9052 - acc: 0.4485 - val_loss: 1.9575 - val_acc: 0.4410\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8105 - acc: 0.4731 - val_loss: 1.9057 - val_acc: 0.4548\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7126 - acc: 0.5006 - val_loss: 1.8791 - val_acc: 0.4674\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.6129 - acc: 0.5246 - val_loss: 1.8370 - val_acc: 0.4803\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.4978 - acc: 0.5554 - val_loss: 1.8186 - val_acc: 0.4946\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.3848 - acc: 0.5850 - val_loss: 1.8443 - val_acc: 0.4830\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2632 - acc: 0.6233 - val_loss: 1.8433 - val_acc: 0.4978\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.1315 - acc: 0.6630 - val_loss: 1.8736 - val_acc: 0.4857\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.9949 - acc: 0.7014 - val_loss: 1.9203 - val_acc: 0.4861\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8794 - acc: 0.7366 - val_loss: 1.9559 - val_acc: 0.4933\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.7557 - acc: 0.7700 - val_loss: 2.0227 - val_acc: 0.4919\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6528 - acc: 0.8025 - val_loss: 2.0961 - val_acc: 0.4794\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.5596 - acc: 0.8380 - val_loss: 2.1543 - val_acc: 0.4915\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.4713 - acc: 0.8621 - val_loss: 2.2746 - val_acc: 0.4799\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4034 - acc: 0.8815 - val_loss: 2.3406 - val_acc: 0.4835\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3462 - acc: 0.9028 - val_loss: 2.3916 - val_acc: 0.4879\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2946 - acc: 0.9181 - val_loss: 2.4468 - val_acc: 0.4870\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2622 - acc: 0.9286 - val_loss: 2.5483 - val_acc: 0.4665\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2275 - acc: 0.9390 - val_loss: 2.5753 - val_acc: 0.4794\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2092 - acc: 0.9429 - val_loss: 2.6601 - val_acc: 0.4741\n",
      "Epoch 26/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1834 - acc: 0.9504 - val_loss: 2.6506 - val_acc: 0.4812\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1805 - acc: 0.9495 - val_loss: 2.7751 - val_acc: 0.4785\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1585 - acc: 0.9567 - val_loss: 2.7925 - val_acc: 0.4745\n",
      "Epoch 00028: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.9911 - acc: 0.2110 - val_loss: 2.6931 - val_acc: 0.2755\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.4789 - acc: 0.3250 - val_loss: 2.3549 - val_acc: 0.3430\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2811 - acc: 0.3593 - val_loss: 2.2496 - val_acc: 0.3855\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2049 - acc: 0.3874 - val_loss: 2.1830 - val_acc: 0.4025\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.1813 - acc: 0.4099 - val_loss: 2.5470 - val_acc: 0.4056\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.9690 - acc: 0.4396 - val_loss: 1.9723 - val_acc: 0.4423\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.8334 - acc: 0.4675 - val_loss: 1.9113 - val_acc: 0.4499\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.7458 - acc: 0.4912 - val_loss: 1.8638 - val_acc: 0.4669\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.6482 - acc: 0.5175 - val_loss: 1.8499 - val_acc: 0.4727\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.5507 - acc: 0.5369 - val_loss: 1.8239 - val_acc: 0.4745\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.4430 - acc: 0.5698 - val_loss: 1.8174 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.3392 - acc: 0.5990 - val_loss: 1.8269 - val_acc: 0.4973\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.2091 - acc: 0.6371 - val_loss: 1.8889 - val_acc: 0.4785\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.0906 - acc: 0.6719 - val_loss: 1.8924 - val_acc: 0.4969\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.9833 - acc: 0.7006 - val_loss: 1.9166 - val_acc: 0.5004\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.8795 - acc: 0.7306 - val_loss: 1.9691 - val_acc: 0.4875\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.7814 - acc: 0.7619 - val_loss: 2.0401 - val_acc: 0.4897\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6836 - acc: 0.7903 - val_loss: 2.1004 - val_acc: 0.4799\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6102 - acc: 0.8140 - val_loss: 2.1389 - val_acc: 0.4839\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 0.5286 - acc: 0.8416 - val_loss: 2.1998 - val_acc: 0.4870\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4718 - acc: 0.8601 - val_loss: 2.2694 - val_acc: 0.4946\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4066 - acc: 0.8790 - val_loss: 2.3399 - val_acc: 0.4924\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3597 - acc: 0.8926 - val_loss: 2.3755 - val_acc: 0.4839\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3245 - acc: 0.9035 - val_loss: 2.4193 - val_acc: 0.4875\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2888 - acc: 0.9161 - val_loss: 2.5318 - val_acc: 0.4852\n",
      "Epoch 26/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2694 - acc: 0.9211 - val_loss: 2.5200 - val_acc: 0.4951\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2434 - acc: 0.9284 - val_loss: 2.6330 - val_acc: 0.4857\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2237 - acc: 0.9349 - val_loss: 2.6470 - val_acc: 0.4879\n",
      "Epoch 29/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2189 - acc: 0.9354 - val_loss: 2.6484 - val_acc: 0.4835\n",
      "Epoch 00029: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 77s 4ms/sample - loss: 2.9944 - acc: 0.2134 - val_loss: 2.6633 - val_acc: 0.2786\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.4989 - acc: 0.3190 - val_loss: 2.3626 - val_acc: 0.3453\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.3280 - acc: 0.3574 - val_loss: 2.3248 - val_acc: 0.3761\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2743 - acc: 0.3847 - val_loss: 2.2233 - val_acc: 0.3998\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.1595 - acc: 0.4084 - val_loss: 2.0494 - val_acc: 0.4137\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9781 - acc: 0.4251 - val_loss: 1.9959 - val_acc: 0.4141\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8943 - acc: 0.4497 - val_loss: 1.9433 - val_acc: 0.4419\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8104 - acc: 0.4724 - val_loss: 1.9104 - val_acc: 0.4589\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7279 - acc: 0.4900 - val_loss: 1.8599 - val_acc: 0.4794\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.6472 - acc: 0.5152 - val_loss: 1.8452 - val_acc: 0.4776\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.5553 - acc: 0.5420 - val_loss: 1.8437 - val_acc: 0.4732\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.4544 - acc: 0.5674 - val_loss: 1.8177 - val_acc: 0.4875\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.3615 - acc: 0.5945 - val_loss: 1.8391 - val_acc: 0.4830\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2592 - acc: 0.6216 - val_loss: 1.8569 - val_acc: 0.4794\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.1541 - acc: 0.6543 - val_loss: 1.8636 - val_acc: 0.4839\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.0514 - acc: 0.6810 - val_loss: 1.8894 - val_acc: 0.4857\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.9467 - acc: 0.7134 - val_loss: 1.9431 - val_acc: 0.4790\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8724 - acc: 0.7298 - val_loss: 1.9888 - val_acc: 0.4776\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7913 - acc: 0.7558 - val_loss: 2.0193 - val_acc: 0.4888\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7093 - acc: 0.7831 - val_loss: 2.1060 - val_acc: 0.4843\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.6393 - acc: 0.8033 - val_loss: 2.1333 - val_acc: 0.4799\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.5724 - acc: 0.8235 - val_loss: 2.2034 - val_acc: 0.4817\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.5222 - acc: 0.8410 - val_loss: 2.2798 - val_acc: 0.4745\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4780 - acc: 0.8492 - val_loss: 2.3422 - val_acc: 0.4772\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4222 - acc: 0.8710 - val_loss: 2.3686 - val_acc: 0.4776\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3969 - acc: 0.8775 - val_loss: 2.4419 - val_acc: 0.4799\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3617 - acc: 0.8875 - val_loss: 2.4361 - val_acc: 0.4821\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3303 - acc: 0.8973 - val_loss: 2.5147 - val_acc: 0.4741\n",
      "Epoch 29/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3063 - acc: 0.9073 - val_loss: 2.5394 - val_acc: 0.4803\n",
      "Epoch 30/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2937 - acc: 0.9119 - val_loss: 2.5923 - val_acc: 0.4781\n",
      "Epoch 31/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2725 - acc: 0.9186 - val_loss: 2.6167 - val_acc: 0.4758\n",
      "Epoch 32/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2636 - acc: 0.9185 - val_loss: 2.6633 - val_acc: 0.4767\n",
      "Epoch 33/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2376 - acc: 0.9290 - val_loss: 2.6864 - val_acc: 0.4830\n",
      "Epoch 34/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2300 - acc: 0.9281 - val_loss: 2.7175 - val_acc: 0.4691\n",
      "Epoch 35/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2249 - acc: 0.9299 - val_loss: 2.7093 - val_acc: 0.4718\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Define parameters  -\n",
    "units = 512  # From the mentioned article in data science\n",
    "epochs = 100  # Just a large number since I am using early stopping\n",
    "validationSplit = 0.1 # My data set is small so I want to use as much as possible to train vs. validate\n",
    "shuffle = True\n",
    "batchSize = 64 # Doubled the default batch size to speed up training\n",
    "dropOut = .5  # http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "\n",
    "\n",
    "for i in range(2,6):\n",
    "    d = i*0.1\n",
    "    countryTrain(units,epochs,validationSplit,shuffle,batchSize,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that encodes a kickoff text string and then plugs it into our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_lyric, n_chars):\n",
    "    lyrics = seed_lyric\n",
    "    for __ in range(n_chars):\n",
    "    # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in lyrics]\n",
    "    # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "    # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "    # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "    # append to input\n",
    "        lyrics += char\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a song other than Stairway to Heaven that I could have used for the kickoff sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLyrics = \"I've had a largemouth bass bust my line A couple beautiful girls tell me, Goodbye Trucks break down,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLyrics = \"There's a lady who's sure All that glitters is gold And she's buying a stairway to heaven When she g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = open('lyrics.Coolio.Epoch100.txt','w')\n",
    "\n",
    "model = load_model('model.100Epoch_6.6.best')\n",
    "lyricsFinal = generate_seq(model,mapping,length,startLyrics,1000)\n",
    "__ = open('lyrics.Coolio.Epoch100.txt','a+')\n",
    "#__.write('Drop {}\\n\\n'.format(modelNum))\n",
    "__.write(lyricsFinal)\n",
    "__.write('.\\n\\n\\n')\n",
    "__.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and print the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(modelNum):\n",
    "    model = load_model('model.drop_0.{}.best'.format(modelNum))\n",
    "    lyricsFinal = generate_seq(model,mapping,length,startLyrics,1000)\n",
    "    __ = open('lyrics.LedZep.txt','a+')\n",
    "    __.write('Drop {}\\n\\n'.format(modelNum))\n",
    "    __.write(lyricsFinal)\n",
    "    __.write('.\\n\\n\\n')\n",
    "    __.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = open('lyrics.LedZep.txt','w')\n",
    "__.write('Stairway to Heaven/n/n')\n",
    "__.close()\n",
    "\n",
    "for i in range(0,6):\n",
    "    run_models(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_json('modelLyricsHistory.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-country-song",
   "language": "python",
   "name": "nlp-country-song"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
