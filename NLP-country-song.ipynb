{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 2em; font-weight:bold\">AI 70's Country</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import tensorflow\n",
    "#import keras\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense,LSTM,Dropout\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "import string\n",
    "\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, data prep is the hardest part of the project.  Because I am going to use a validation set in training my model, and because Keras uses the last n% of the data as the validation set, I want to shuffle the lyrics so that my validation set contains a better representation of all the data - not just the last song.  I also want to get the most originality that I can out of the model, so I will eliminate duplicate lyrics (some songs have refrains that repeat multiple times).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricsCSV = pd.read_csv('lyricsTrain_35.csv',encoding='ISO-8859-1')\n",
    "lyricsCSV.sort_values(lyricsCSV.columns[0],inplace=True)\n",
    "lyricsCSV.drop_duplicates(keep='first',inplace=True)\n",
    "lyricsCSV = lyricsCSV.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "lyricsCSV.to_csv('lyrics.txt',sep='\\t',index=False)\n",
    "l = open('lyrics.txt','r')\n",
    "lyrics = l.read()\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove lines and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics That makes a body feel alone And thank God you're a country boy There's nothin' left for me to say-ay Cause she walks dowtown with a suitcase in her hand How you could easily take my man If I were a carpenter Your beauty is beyond compare You put me high upon a pedestal My daddy worked all night in the Van Lear coal mines But if that's what it takes to hold you Hope you're doin' fine Just for old time's sake Tell me you love me and don't let me cry Say anything but don't say goodbye And I stopped beside the Sunday school As the tears fell on his daddy's face, he heard these words again People writing letters back home To lose these memories It's too sad to write My happiness depends on you Well, my fiddle was my daddy's till the day he died And I can't believe it's you Thank God I'm a country boy \"\"\"Ninety days, Jerry, when you hot, you hot\"\"\" Who claims that he just don't believe in fightin' And there's nothing I can do to keep A lotta sad people thinkin' that's mighty keen \"\"\"Judge\"\"\" When the party's all over she'll welcome him back home again But in the wintertime we'd all get a brand new pair Cause therels something in a sunday Your voice is soft like summer rain So take away the flowers that you gave me Younger than the mountains, blowing like a breeze And he took me by the hand and held me close to his side Now I'm the one sleeping all alo-oh-one Give me no reasons, give me alibies And don't it make my brown eyes Save your love through sorrow Jolene, Jolene, Jolene, Jolene Let me kiss you Sleepin' in our king size bed You can 'splain it all down at City Hall As you used to be No one will ever know how much I love you so I haven't slept a wink all night long Got a whole lot a good woman Well there goes my only possession I'd be a fool 'cause I finally found someone who really cares And found my cleanest dirty shirt Cause we both know that I'm not \"He said, \"\"this one's for Becky, as he watched the last one fall\" Something always told me they were reading Tommy wrong Just like your daddy is What's that darlin' But it sure seems warmer than it did I remember well, the well where I drew water And I'm torn between the things that I should do Back to his arms and never know You make the coffee Four car garage and we're still building on Take me home, down country roads He turned around and grinned at me and said I will find a way, find a way And it's good when I finally make it home, all alone Through teardrops and laughter, they'll pass through this world hand-in-hand, Money made from selling a hog Tomorrow she'll probably want me back I'll think of you each step of the way Let the devil take tomorrow People see us everywhere they all think you really care Half as lonesome as the sound I guess I shouldn't say anything at all since you're supposed to belong to him. Taught me how to work and play a tune on the fiddle Didn't know just what I had I read about some squirrelly guy I realise the way your eyes deceive me \"I said \"\"Hey, judge, old buddy, old pal\"\"\" There's been a load of compromisin' And sometimes all the nights can be so long And there's nothing short of dying They're walkin' on the fightin' side of me But you don't know what he means to me Cries for you each night Honey it's almost nine When he lo-oves me, he really lo-oves me But like a big red rose that's made of paper I'm sleeping single in a double bed (ah-hah-ah) I-I'm sleeping single in a double bed (ooh-oo-ooh) The Gatlin boys just laughed at him when he walked into the barroom But what would it matter I'd pour me a drink, but I'd only be sorry This successful life we're livin' You're hot Twenty years of crawling was bottled up inside him Your smile is like a breath of spring I won't try to understand Misty taste of moonshine, teardrop in my eye That's what the trouble was You better look before you leap, still waters run deep I've been layin' in bed thinkin' like a woman And a mill wheel grinding If I swore you were an angel, To forgive me Sleeping single in a double bed I'd just as soon let you go Back ever since the world began And stumbled down the stairs I beg your pardon Tossing, turning, trying to forget (you-ou) To face the world out on my own again How's your new love Cause there's something in a sunday You gave me strength to stand alone again And if you should ever I'm doin' alright To think he might be out with another woman Jolene, Jolene \"\"\"Son, my life is over, but yours has just begun\"\"\" Wonderin' if my man's been doin' me wrong And I wish you happiness The judge was a fishin' buddy that I recognized When you're hot, you're hot Jolene Well me and Homer Jones and Big John Talley They wonder how does a man get to be this way I never was one of them money hungry fools You look into my eyes and lie those pretty lies A doctor and a lawyer man This successful life we're livin's got us feudin' And it's so good to be back home again Goodbye darlin' That they were singing If I should stay Til the early mornin' light You waltz right in the door I sold my soul, you bought it back for me Where you belong to only me. If I worked my hands on wood My hopes my dreams come true my life I give for you They love our milk and honey Somewhere along the way It won't mean you're weak if you turn the other cheek A brown eyed handsome man I looked after Tommy, 'cause he was my brother's son \"\"\"I promised you, Dad, not to do the things you've done\" Well, I was borned a coal miner's daughter I hope you're old enough to understand I'd rather wonder a little and have his lovin' Just when I'm about to make it work without you My only prayer will be some day you'll care for me but it's only make believe He'll probably just ride away The judges wife called up My heart I can't control you lure my very soul Wake up sleepy head Well I got me a fine wife I got me an ole fiddle Cross the sand Honey, come back where you belong to only me. There's someone for everyone, and Tommy's love was Becky If you were a carpenter You're walkin' on the fightin' side of me He's the only one for me But I just can't let you go without telling you just how much I love you. And there got all my defenses I would give you the world right now on a silver platter And did I hear you say he was a-meeting you here today Cause she's a good-hearted woman; she loves her good timin' man Little warm puppies and children and girls of the night But above all this And who knows, maybe, on some special night, if my song is right Like the Hatfield and McCoys Then I fumbled through my closet Newberry's train songs and Blue Eyes Cryin' in the Rain How it would feel to say When you're runnin' down our country, hoss They been walking thirty miles Walk with me world Than what I've been fighting at home It was almost like a song Would your flowing love come quench me You were in my arms And here I go And I can easily understand To the things you seek to find This successful life we're livin' got us feudin' Willie and Waylon and the boys Well people may try to guess, the secret of my happiness I got to go now Comeback darlin' But his pride won't let him do things to make you think he's right (La la la la la la la) (La la la la la) And listened to the song I don't want to be alone Life ain't nothin' but a funny funny riddle But tonight I need a friend She had the world Just one more minute Just when I've begun to get myself together Now my broken heart Free that brown eyed man Thinking over things I wish I'd said (ooh-ooh) And I cry all night 'til dawn A man could wake up dead And held me up and gave me dignity I wish you joy We've been so busy keepin' up with the Jones Would you marry me anyway There goes my everything Back when ever since the world began And I wonder just how long He ain't wrong he's just different Just to know means so much to me Now you be careful My hopes my dreams come true my one and only you And my soft shoes shining Makes my temper rise with jealousy Give me your tomorrow Her smile told of no night There goes my reason for living When you take you gotta give so live and let live It's been a long time Hello darlin' In your high society you cry all day \"He said, \"\"Come on out and say what's on your mind\"\"\" He loved me all the way-ay Gotta go, I love you Til the sunlight shines through your face But you could've heard a pin drop when Tommy stopped and locked the door Crying Lisa, Lisa This coat and tie is choking me \"I said \"\"well I'll tell ya one thing judge, old buddy, old pal\"\"\" But when they're runnin' down our country, man A long time forgotten are dreams that just fell by the way From crying when he calls your name I guess he needs some time away from me From a mail order catalog Why I've seen her fingers bleed Thank you darlin' All day long in the field a hoin' corn But the Lord and my wife wouldn't take it very good To meet the day But if you ever want somebody to just love ya, and some day you I hear her voice, in the morning hour she calls me Wish she hadn't done me that way And gripin' 'bout the way things oughta be The joy of love that used to taste like Oh there goes my everything Tell you 'bout my friends out on the coast Ain't much an old country boy like me can't hack Take me home, country roads Delta Dawn, what's that flower you have on? But she has faith in me, and so I go on trying faithfully It's early to rise, early in the sack Lonestar belt buckles and old faded Levi's and each night begins a new day She's a good-hearted woman in love with a good-timin' man Sleeping under a table at a road side park Don't know when I've been so blue As much as I love waking up next to you Runnin' down the way of life There goes the one of my dreams Mommy scrubbed our clothes on a washboard ever' day Honey, come back Now please don't think I'm weak, I didn't turn the other cheek Of a love so warm and true Yeah, city folk drivin' in a black limousine I hear footsteps slowly walking \"He said \"\"Well, when you're hot, you're hot\"\"\" Gettin' card and letters from people I don't even know I'll be fine when you're gone Wind whipping down the neck of my shirt And I was just gettin' ready to roll 'em again And there won't always be someone there to pull you out Them that don't know him won't like him But I'd rather fight the wind and rain Darling this will be goodbye for evermore Thinking over things I wish I'd said (fade) There goes my only possession She fought and won herself There'll be a load of compromisin' (La la la la la la la, when you're hot, you're hot) I said I'd never say it again She was your morning light Is here you come again, and here I go, here I go All my memories gather round her \"\"\"Promise me, son, not to do the things I've done\" All the folks around Brownsville say she's crazy I got to try to find a way Don't it make my brown eyes When I was lost you took me home Please don't take him even though you can Except the memory of a coal miner's daughter Yonder comes a truck with the US mail I say, yeah, when you're hot, you're hot May God bless you And I shaved my face Three thousand years Lord knows she don't understand him, but she does the best that she can I told her someday, if she was my girl, I could change the world And when you're not, you're not I don't care what's right or wrong And turned my lies back into truth again All I'm takin' is your time It sure is cold today But I could never love again Mommy rocked the babies at night Sometimes when he comes home I'm cookin' breakfast Hello sunshine I would only be in your way Riding out on a horse in a star-spangled rodeo \"\"\"I'll pay ya that hundred I owe ya if you'll get me outta this spot\"\"\" And a smile can hide all the pain Shedding tears for I still recall the final words my brother said to Tommy Now we can talk all night about the weather Has been shattered by the closing of the door Here I go And I love you will always love you Well, I guess that's about all I gotta say. Cause when he loves me he loves me a-all the way In the park I saw a daddy \"Son, you don't have to fight to be a man\"\"\" I could promise you things like big diamond rings But myself I can't deceive I know it's only make believe Lay it soft upon my skin As they gently walk across the lonely floor \"\"\"Pay for my Cadillac?\"\"\" Well if sweet-talkin' you could make it come true Now tell me the truth When you're runnin' down our country, man Are you happy? Caught the sunday smell Then I crossed the empty street and I got my song and I got you with me tonight One more time He was swingin So smile for a while and let's be jolly He wasn't holding nothin' back, he let 'em have it all Shine on me sunshine Mamma told her daughter And let her know you think about her when you're gone If you don't understand him and he don't die young Runnin' down a way of life \"\"\"You understand that, you hillbilly?\"\"\" A brown eyede handsome man Getting cards and letters from people I don't even know Well I wouldn't trade my life for diamonds and jewels The disappearing dreams of yesterday I gave you my onlyness Now shine on me sunshine Yeah, I'm proud to be a coal miner's daughter And nice guys get washed away like the snow and the rain Way back in history Or I could ask what I really want to know But it's much too sad to write That I always thought it could be So high that I could almost see eternity Oh when he lo-oves me, he really lo-oves me Then I headed back for home For my clothes Daddy always managed to get the money somewhere I'd marry you anyway Then the flame became a dying ember And I'm so sorry And it echoed thru the canyon like Between Hank Williams' pain songs and She just talks about the good times they've had and all the good times to come One more hug would do I know every crack in these dirty sidewalks of Broadway I walk away from trouble when I can With tender looks that I mistook for love I hope life, treats you kind That I did you wrong Your love for her grew You've found someone new and So if I said you had a beautiful body I-Im sleeping single in a double bed Just waiting for me like a secret friend, and there's no end I told her someday if she was my girl, I could change the world With my little songs, I was wrong That I should have been home yesterday, yesterday I've always got a smiling face, anytime & any place I love you and I miss you If you don't love it, leave it That's the one thing that daddy made sure of \"You even called me \"\"friend\"\"\" Save your love through loneliness I'd have your baby Just leave it up to you and in a little while And here is what I'll say Beautiful daughter couldn't His hand led hers away He was only ten years old when his daddy died in prison We were poor but we had love While she lays crying, I fumble with a melody or two In the palm of her hand Mamas' don't let your babies grow up to be cowboys He like the night life, the bright lights and good-timin' friends Bad so I had one more for dessert Just may, just give me a call-you know where I am \"n' I said \"\"Thanks a lot\"\"\" I could be lying with you instead I needed you and you were there I'll fix your lunch You're messin' up my mind and fillin' up my senses Mama don't let your babies grow up to be cowboys What you need Another sleepless night and it's the same old stor-ory Life is old there, older than the trees \"But daddy always told me, \"\"Don't make small talk\"\"\" About the way they have to live here in this country I'd not miss my colored blouse Well life on the farm is kinda laid back In her arms, he didn't have to prove he was a man Or let go oh-whoa-whoa-whoa His mama named him Tommy, but folks just called him yellow And the beer I had for breakfast wasn't Blue Ridge Mountains, Shenandoah River I never thought of ever leaving Butcher Holler And I don't mind 'em switchin' sides Where hustle's the name of the game But when he lo-oves me, he really lo-oves me Had a big crap game goin' back in the alley On the sunday morning sidewalk Out in Luckenbach, Texas ain't nobody feelin' no pain Cowboys like smokey old pool rooms and clear mountain mornin's Tommy opened up the door, and saw his Becky crying Each lonely day's a little bit longer \"I said \"\"Yeah?\"\"\" And shakin' me up so that all I really know Would you have my baby? Take the ribbon from my hair I never promised you a rose garden So I fiddle when I can, work when I should Everyone considered him the coward of the county You held my hand when it was cold And Jerry Jeff's train songs and Blue Eyes Cryin' in the Rain On the road to my horizon Sleeping single in a double bed (ah-hah-ah) There's gotta be a little rain some time Just like you've done before \"Whadda you mean 'contempt of court'?\"\"\" Once in every life On the sleeping city sidewalk Prettiest woman you ever laid eyes on Til the sunlight has touched your face Oh how real those roses seem to me If I said you had a beautiful body And whatever you decide to do And I'll never leave, why should I leave? \"\"\"If you wasn't wearin' that black robe I'd take out in back of this courthouse\" In a cabin, on a hill in Butcher Holler So he gave my friends a little fine to pay That you ever dreamed of I can see the happy years we've had before Love shouldn't be so melancholy \"\"\"Who gonna collect my welfare?\"\"\" Please don't take him just because you can That's why we moved it With cigarettes and songs And you fix mine No one will ever know just how much I love you so He was sitting in the witness stand Taught me how to love and how to give just a little Like your imitation love for me I'll be waiting for you I will find a way I, I will always, always love you And promised her he'd take her for his bride All at once you weren't here With a subway token and a dollar tucked inside my shoe Well a lot of things have changed since a way back then And tomorrow's out of sight Big fine cars and fancy clothes. Why'd we move that bojangle clock so far away from the bed With flaming locks of auburn hair But they're only imitation Well a simple kinda life never did me no harm In route to L.A. to get To take you to his mansion in the sky? Now the love that kept this old heart beating \"When Tommy turned around they said, \"\"hey look, old yeller's leaving\"\"\" Sleeping single in a double bed (ooh-oo-ooh) And ever' thing would start all over come break of morn I was confused, you cleared my mind Would you hold it against me Country roads, take me home Rhinestone cowboy The only two things in life that make it worth livin' Wishing lord that I was stoned It's a skippidity do da day All you gotta do is smile that smile And I will always love you And offers comin' over the phone Gettin' cards and letters from people I don't even know And standin' up for things they believe in There isn't any sweetness in your heart \"And keep all that money for evidence\"\"\" Well, when he took us inta court I couldn't believe my eyes So that is why I'm gonna say it one more time But I'm gonna be where the lights are shinin' on me I'm happiest girl, in the whole U.S.A You're just as lovely Right where you belong Let me hold you in my arms Shake it loose and let it fall Paper roses, paper roses, God, her love is true I could sing you a tune and promise you the moon \"(When you're hot, you're hot\"\")\" I've been walkin' these streets so long My eyes are not blue Help me make it through the night Flying cross the desert in a TWA Look up darlin' And so I'll go, but I know I had to have this talk with you Out in Luckenbach, Texas there ain't nobody feelin' no pain Don't let 'em pick guitars and drive them old trucks When Tommy left the barroom, not a Gatlin boy was standing And read the Bible by the coal oil light My heart or wedding ring my all my everything And every time they ask me why I just smile & say With a laughing little girl But she lost both her arms In the summertime we didn't have shoes to wear That I'd lost somehow I thought that you would be a perfect lover I'll just cry all night long And we were so in love If a tinker was my trade And It's almost like a song Don't know what's come over you You could have your choice of men Like the shadows on the wall My plans my hopes my schemes you are my every dream but it's only make believe Except I can't sleep While she lays, while she waits for me She'd smile in mommy's understanding way The torn dress, the shattered look was more than he could stand My only prayer will be that some day you'll care for me but it's only make believe Been a whole lot a good women The kids are asleep so I keep it kinda low Like a rhinestone cowboy They took turns at Becky, n'there was three of them Honey, come back I just can't stand I'm not ever gonna worry about tomorrow Looking for a mysterious dark-haired man I'd smoked my brain the night before Find it in your heart I'm not Lisa, my name is Julie And it took me back to something Thank you oh Lord for making him for me \"Said, \"\"Live a good life and play the fiddle with pride\" The district attorney said you better Here you come again Go out and find herself My daddy taught me young how to hunt and how to whittle Would you still love me? I jest kept on rollin' and controllin' them bones Kiss an angel good morning Cussin' at a can that he was kickin If you were a miller If you want your job you'd better Come and lay down by my side When the sun's comin' up I got cakes on the griddle Forget I've ever known her Let's go to Luckenbach, Texas Good-bye, please don't cry Where you've stayed for years Of someone fryin chicken And each step brings you closer And you were a lady \"And I come to her and say, \"\"it was all right, \"\" and I hold her tight\" And love her like the devil when you get back home I'd answer you yes I would But all these years I've never caught him cheatin' Rain dripping off the brim of my hat In a wrestling match to get They'd rather give you a song then diamonds or gold Following behind you Say it isn't true and That's all I'm taking with me \"\"\"Look like I'm gonna hafta haul you all in\" Milo Venus was a beautiful lass But honey now I do I'd rather have my fiddle and my farmin' tools And I kept rollin' them sevens , winnin' all them pots And pretty soon I'm wond'rin how I came to doubt you. Have a beautiful day, Now, it won't mean you're weak if you turn the other cheek And somewhere far away Tell me no secrets, tell me some lies January through December And would you not be above me? And thank you for letting life turn out the way And papa, I sure hope you understand And I cannot compete with you Nice to see you While she lays sleeping, I stay out late at night and play my songs Then I see my old guitar in the night But you don't find roses growin' on stalks of clover He shoveled coal to make a poor man's dollar Walk away from trouble if you can Then a man of low degree stood by her side When you're the only one at two in the morning Maybe it's time we got back to the basics of love With no way to hold my head With ivory skin and eyes of emerald green Seems like a hundred years ago Let this song that I'm singin' be a warnin' The rest of us can count on bein' free While she lays waiting, I stumble to the kitchen for a bite And there go all my defenses He talks about you in his sleep As long as he makes everything alright today-ay Not much left but the floor, nothing lives here anymore Or Phoenix Arizona That didn't hurt Got us feuding like the Hatfields and McCoys Do these old shoes look funny Don't let 'em pick guitars or drive them old trucks \"\"\"When you hot, you hot\"\"\" But some of them never learn it's a simple thing \"I said, \"\"Well, son when you hot, you hot\"\"\" To complain, there was no need And a voice is softly saying My luck was so good I could do no wrong Somehow you needed me And here I am'a walking down sixty-six She loves him in spite of his ways that she don't understand So I'm just gonna take my bags and I'm gonna walk. The radio reminds me of my home far away And send the kind that you remind me of You seemed so full of sweetness at the start West Virginia, mountain mama Our fightin' men have fought and died to keep I could by lying with you instead Let 'em be doctors and lawyers and such Well, I really don't mind the rain Is guitars that tune good and firm feelin' women But she never complains of the bad times or bad things he's done, Lord When ever I chance to meet, some old friends on the street He'd never stood one single time to prove the county wrong A good-hearted woman loving her good timing man There once was a time when I could not imagine At night we'd sleep 'cause we were tired If I were dying of thirst My days are all filled with an easy country charm And finally they jest threw up their hands and said I'm sleeping single in a double bed Thinking over things I wish I'd said (ahh-ah) And kiss the happiest girl, in the whole U.S.A. And I dream of the things I'll do She left you here drowning in your tears, here And then one winter day Cause drinking doubles alone, don't make it a par-arty I'll make the bed Well I woke up Sunday morning You needed me, you needed me I can't believe it's true Singin' the same old song The secret I'm speaking of, is a woman & a man in love Come along and share the good times while we can Almost heaven, West Virginia What I'm tryin' to say is that Even with someone they love So you better think it over One day while he was working, the Gatlin boys came calling One of them got up and met him halfway 'cross the floor A raisin' me a family and workin' on a farm And wrap my heart 'round your little finger I hear people talkin' bad, He let my friends go free and throwed the book at me When I heard somethin' behind me Someone comes along I cried a tear, you wiped it dry And she says to wake her up when I am through But you're down when you're ridin' the train that's takin' the long way Buy some boots and faded jeans and go away We had such a perfect year Arrested on charges Lisa left you years ago And the answer is in this song that I always sing I should have held you but I let you go As my memory turns back the pages \"He said \"\"Yeah\"\"\" I'm the happiest girl, in the whole U.S.A. Of unemployment Saw a women walking I could ask a lot of crazy questions Don't it make my brown eyes blue I will always love you Miner's lady, stranger to blue water Son, let me tell ya now exactly what I mean Along with the sunshine Cause they'll never stay home and they're always alone \"He said \"\"Hello, boys\"\" and then he gave us a grin 'n' said\" Make up her mind between To the place I belong Put all the money in and let's roll 'em again A lonely bell was ringing He reached above the fireplace, and took down his daddy's picture Yesterday is dead and gone My one and only prayer is that some day you'll care Harpin' on the wars we fight That I've been pickin' How am I doin'? I'd be carrying the pots you made When the work's all done and the sun's settlin' low Skippidity do da I don't need my name in the marquee lights Dark and dusty, painted on the sky In her younger days they called her Delta Dawn I know those bright lights are callin' ya, honey. Is anybody goin' to San Antone \"Sometimes you gotta fight when you're a man\"\"\" And them that do sometimes won't know how to take him But there's one thing I want you to know Would I still find you Could it be a faded rose from days gone by? And you know what I'm talkin' about And you came to me Sunday morning coming down Good morning morning And your soft shoes shining? If I were a miller While she lays dreaming, I try to get undressed without the light But I lit my first and watched a small kid I'm begging of you please don't take my man The sun can shine so bright up in the sky Between Hank Williams' pain songs Here you come again lookin' better than a body has a right to And driving down the road I get a feeling But they preach about some other way of livin' Would you miss your colored blouse I turned around and there was a big old cop You gave me hope when I was at the end \"And quietly she says \"\"how was your night?\"\"\" Cowboys ain't easy to love and they're harder to hold And who knows maybe on some special night, if my song is right I wish you love I didn't mean to treat you bad And it's sad to be alone Bitter-sweet memories And combed my hair \"\"\"And I'd try a little bit of your honor on\"\"\" Like I aint got nothing on And I hope that you have all Here you come again and here I go The good life he promised ain't what she's living today So baby, let's sell your diamond ring And I were a lady You've got to kiss an angel good morning Well, now every time I rolled them dice I'd win Anyplace is alright as long as I \"She's forty-one and her daddy still calls her \"\"baby\"\"\" Would you treat me like the devil tonight Honey, I know I've said it too many times before I pull out my fiddle and I rosin up the bow With Waylon and Willie and the boys But I'll still be just as gone The destination was The work we done was hard But mine won't leave you Is here you come again and here I go Than the last time I held you And she believes in me, I'll never know just what she sees in me Daddy loved and raised eight kids on a miner's pay I'd play Sally Goodin all day if I could The rain can fall so soft against the window Do you love waking up next to me Tossing, turning, trying to forget (ahh-ah) Thinking over things I wish I'd said Can forget I've ever known her With each rising sun\n"
     ]
    }
   ],
   "source": [
    "tokens = lyrics.split()\n",
    "lyrics = ' '.join(tokens)\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing, when I look at the above lyrics, I seem to see a LOT of quotation marks.  So, I am going to just replace those with a space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = lyrics.replace('\"',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build sequences of characters that will be used to predict a final character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100 # Length of the characer sequences (because we have so much verbage,\n",
    "             # we can use a relatively large number)\n",
    "sequences = list()\n",
    "for i in range(length, len(lyrics)):\n",
    "    seq = lyrics[i-length:i+1]\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save a .txt file of our sequences with line endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '\\n'.join(sequences)\n",
    "file = open('char_sequences.txt','w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of character:number mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('char_sequences.txt','r')\n",
    "raw_text = file.read()\n",
    "file.close()\n",
    "\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Save the mapping as json \n",
    "json_map = json.dumps(mapping)\n",
    "__ = open('mapping.json','w')\n",
    "__.write(json_map)\n",
    "__.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dictionary to create sequences of numbers only (numbers that describe the characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list()\n",
    "for line in lines:\n",
    "    encoded_seq = [mapping[char] for char in line]\n",
    "    sequences.append(encoded_seq)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input sets (with 99 characters) and output sets (1 character) and then one-hot code the sets so we can use them to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X] #one-hot code input\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size) #one-hot code output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with tuning parameters determined by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def countryTrain(paramUnits,paramEpochs,paramValSplit,paramShuffle,paramBatchSize,paramDropout):\n",
    "\n",
    "    #Define callbacks\n",
    "    es = EarlyStopping(monitor = 'acc',min_delta = .01, patience = 2, mode = 'max',verbose=1)\n",
    "    mc = ModelCheckpoint('model.drop_{}.best'.format(str(paramDropout)), monitor='loss', mode='min', save_best_only=True) # Keep best model\n",
    "\n",
    "\n",
    "    # define and fit model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(paramUnits, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(paramDropout))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    modelLyrics = model.fit(X, y, epochs = paramEpochs, validation_split = paramValSplit, \n",
    "                            shuffle = paramShuffle, batch_size = paramBatchSize, verbose=1,callbacks=[es,mc])\n",
    "\n",
    "   \n",
    "    history = pd.DataFrame(modelLyrics.history)\n",
    "    history.to_csv('modelLyricsHistory_drop_{}.csv'.format(str(paramDropout)),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26265 samples, validate on 2919 samples\n",
      "Epoch 1/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 2.7718 - acc: 0.2607 - val_loss: 2.3467 - val_acc: 0.3570\n",
      "Epoch 2/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 2.3820 - acc: 0.3552 - val_loss: 2.1293 - val_acc: 0.4018\n",
      "Epoch 3/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.1284 - acc: 0.4005 - val_loss: 1.9944 - val_acc: 0.4282\n",
      "Epoch 4/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 1.9806 - acc: 0.4327 - val_loss: 1.8957 - val_acc: 0.4532\n",
      "Epoch 5/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 1.8626 - acc: 0.4654 - val_loss: 1.8591 - val_acc: 0.4618\n",
      "Epoch 6/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.7549 - acc: 0.4931 - val_loss: 1.8192 - val_acc: 0.4666\n",
      "Epoch 7/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.6467 - acc: 0.5200 - val_loss: 1.7810 - val_acc: 0.4810\n",
      "Epoch 8/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 1.5344 - acc: 0.5494 - val_loss: 1.7715 - val_acc: 0.4865\n",
      "Epoch 9/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 1.4356 - acc: 0.5748 - val_loss: 1.7809 - val_acc: 0.4937\n",
      "Epoch 10/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.3443 - acc: 0.6005 - val_loss: 1.8083 - val_acc: 0.4830\n",
      "Epoch 11/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2648 - acc: 0.6279 - val_loss: 1.7974 - val_acc: 0.4875\n",
      "Epoch 12/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.1943 - acc: 0.6524 - val_loss: 1.7931 - val_acc: 0.4899\n",
      "Epoch 13/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.1299 - acc: 0.6684 - val_loss: 1.8235 - val_acc: 0.4868\n",
      "Epoch 14/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.0740 - acc: 0.6836 - val_loss: 1.8116 - val_acc: 0.4991\n",
      "Epoch 15/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.0258 - acc: 0.7023 - val_loss: 1.8506 - val_acc: 0.4899\n",
      "Epoch 16/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.9987 - acc: 0.7074 - val_loss: 1.8609 - val_acc: 0.4800\n",
      "Epoch 17/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.9647 - acc: 0.7180 - val_loss: 1.8588 - val_acc: 0.4943\n",
      "Epoch 18/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.9296 - acc: 0.7266 - val_loss: 1.8450 - val_acc: 0.5005\n",
      "Epoch 19/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.9043 - acc: 0.7373 - val_loss: 1.8735 - val_acc: 0.4906\n",
      "Epoch 20/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.8706 - acc: 0.7479 - val_loss: 1.8713 - val_acc: 0.4813\n",
      "Epoch 21/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.8571 - acc: 0.7536 - val_loss: 1.8553 - val_acc: 0.4957\n",
      "Epoch 22/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.8334 - acc: 0.7602 - val_loss: 1.8496 - val_acc: 0.4961\n",
      "Epoch 23/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8315 - acc: 0.7604 - val_loss: 1.8695 - val_acc: 0.4926\n",
      "Epoch 24/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8110 - acc: 0.7667 - val_loss: 1.8785 - val_acc: 0.4878\n",
      "Epoch 25/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8155 - acc: 0.7617 - val_loss: 1.8889 - val_acc: 0.4950\n",
      "Epoch 26/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8091 - acc: 0.7695 - val_loss: 1.8710 - val_acc: 0.4974\n",
      "Epoch 27/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8008 - acc: 0.7700 - val_loss: 1.9028 - val_acc: 0.4896\n",
      "Epoch 28/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 0.7926 - acc: 0.7725 - val_loss: 1.9041 - val_acc: 0.4851\n",
      "Epoch 29/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7822 - acc: 0.7761 - val_loss: 1.8972 - val_acc: 0.4889\n",
      "Epoch 30/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7772 - acc: 0.7774 - val_loss: 1.8943 - val_acc: 0.4978\n",
      "Epoch 31/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7745 - acc: 0.7796 - val_loss: 1.8816 - val_acc: 0.4878\n",
      "Epoch 32/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7846 - acc: 0.7713 - val_loss: 1.8489 - val_acc: 0.5053\n",
      "Epoch 33/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7620 - acc: 0.7848 - val_loss: 1.8493 - val_acc: 0.5026\n",
      "Epoch 34/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7821 - acc: 0.7751 - val_loss: 1.8483 - val_acc: 0.5033\n",
      "Epoch 35/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.7937 - acc: 0.7687 - val_loss: 1.8507 - val_acc: 0.4957\n",
      "Epoch 36/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8169 - acc: 0.7635 - val_loss: 1.8886 - val_acc: 0.4930\n",
      "Epoch 37/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8076 - acc: 0.7687 - val_loss: 1.8779 - val_acc: 0.4834\n",
      "Epoch 38/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.7937 - acc: 0.7736 - val_loss: 1.8529 - val_acc: 0.4889\n",
      "Epoch 39/100\n",
      "26265/26265 [==============================] - 185s 7ms/sample - loss: 0.7921 - acc: 0.7750 - val_loss: 1.8649 - val_acc: 0.4841\n",
      "Epoch 40/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.7800 - acc: 0.7816 - val_loss: 1.8623 - val_acc: 0.4923\n",
      "Epoch 41/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8194 - acc: 0.7631 - val_loss: 1.8670 - val_acc: 0.4854\n",
      "Epoch 42/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8986 - acc: 0.7372 - val_loss: 1.8597 - val_acc: 0.4868\n",
      "Epoch 43/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8921 - acc: 0.7419 - val_loss: 1.8313 - val_acc: 0.4967\n",
      "Epoch 44/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 0.8368 - acc: 0.7585 - val_loss: 1.8374 - val_acc: 0.5039\n",
      "Epoch 45/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 0.8583 - acc: 0.7540 - val_loss: 1.8605 - val_acc: 0.4947\n",
      "Epoch 46/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8913 - acc: 0.7415 - val_loss: 1.8646 - val_acc: 0.4943\n",
      "Epoch 47/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.8968 - acc: 0.7384 - val_loss: 1.8188 - val_acc: 0.5019\n",
      "Epoch 48/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.9225 - acc: 0.7318 - val_loss: 1.8403 - val_acc: 0.4892\n",
      "Epoch 49/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.9531 - acc: 0.7239 - val_loss: 1.8452 - val_acc: 0.4964\n",
      "Epoch 50/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.9562 - acc: 0.7214 - val_loss: 1.7949 - val_acc: 0.5087\n",
      "Epoch 51/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 0.9635 - acc: 0.7210 - val_loss: 1.8369 - val_acc: 0.4937\n",
      "Epoch 52/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.1352 - acc: 0.6648 - val_loss: 1.8710 - val_acc: 0.4782\n",
      "Epoch 53/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2910 - acc: 0.6159 - val_loss: 1.7924 - val_acc: 0.5053\n",
      "Epoch 54/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 1.1411 - acc: 0.6613 - val_loss: 1.7877 - val_acc: 0.5005\n",
      "Epoch 55/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.0574 - acc: 0.6910 - val_loss: 1.7779 - val_acc: 0.5057\n",
      "Epoch 56/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 1.0461 - acc: 0.6958 - val_loss: 1.8149 - val_acc: 0.5039\n",
      "Epoch 57/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 1.0805 - acc: 0.6815 - val_loss: 1.8039 - val_acc: 0.4937\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.1493 - acc: 0.6626 - val_loss: 1.7804 - val_acc: 0.5050\n",
      "Epoch 59/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.1432 - acc: 0.6658 - val_loss: 1.8013 - val_acc: 0.4896\n",
      "Epoch 60/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.1106 - acc: 0.6769 - val_loss: 1.8047 - val_acc: 0.4998\n",
      "Epoch 61/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.1787 - acc: 0.6529 - val_loss: 1.8210 - val_acc: 0.4865\n",
      "Epoch 62/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.3081 - acc: 0.6162 - val_loss: 1.8234 - val_acc: 0.4741\n",
      "Epoch 63/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.3508 - acc: 0.5998 - val_loss: 1.7695 - val_acc: 0.4926\n",
      "Epoch 64/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.2800 - acc: 0.6263 - val_loss: 1.7839 - val_acc: 0.4947\n",
      "Epoch 65/100\n",
      "26265/26265 [==============================] - 186s 7ms/sample - loss: 1.2586 - acc: 0.6305 - val_loss: 1.7563 - val_acc: 0.5022\n",
      "Epoch 66/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.1878 - acc: 0.6523 - val_loss: 1.7702 - val_acc: 0.5012\n",
      "Epoch 67/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2040 - acc: 0.6477 - val_loss: 1.7647 - val_acc: 0.5057\n",
      "Epoch 68/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 1.1814 - acc: 0.6572 - val_loss: 1.7586 - val_acc: 0.4998\n",
      "Epoch 69/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.1722 - acc: 0.6560 - val_loss: 1.7690 - val_acc: 0.4964\n",
      "Epoch 70/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2790 - acc: 0.6206 - val_loss: 1.7709 - val_acc: 0.4947\n",
      "Epoch 71/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 1.2451 - acc: 0.6362 - val_loss: 1.7424 - val_acc: 0.5115\n",
      "Epoch 72/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2827 - acc: 0.6304 - val_loss: 2.0407 - val_acc: 0.4265\n",
      "Epoch 73/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.9034 - acc: 0.4481 - val_loss: 1.8805 - val_acc: 0.4615\n",
      "Epoch 74/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.7389 - acc: 0.4913 - val_loss: 1.8324 - val_acc: 0.4666\n",
      "Epoch 75/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.6398 - acc: 0.5180 - val_loss: 1.8032 - val_acc: 0.4813\n",
      "Epoch 76/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.5426 - acc: 0.5450 - val_loss: 1.7691 - val_acc: 0.4896\n",
      "Epoch 77/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.4526 - acc: 0.5756 - val_loss: 1.7421 - val_acc: 0.5050\n",
      "Epoch 78/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.3630 - acc: 0.6037 - val_loss: 1.7305 - val_acc: 0.5115\n",
      "Epoch 79/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2887 - acc: 0.6264 - val_loss: 1.7228 - val_acc: 0.5074\n",
      "Epoch 80/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2708 - acc: 0.6301 - val_loss: 1.7629 - val_acc: 0.5019\n",
      "Epoch 81/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.4692 - acc: 0.5701 - val_loss: 1.8095 - val_acc: 0.4861\n",
      "Epoch 82/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.4793 - acc: 0.5678 - val_loss: 1.7568 - val_acc: 0.5002\n",
      "Epoch 83/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.3632 - acc: 0.6013 - val_loss: 1.7281 - val_acc: 0.5015\n",
      "Epoch 84/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.3008 - acc: 0.6212 - val_loss: 1.7225 - val_acc: 0.5036\n",
      "Epoch 85/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.2754 - acc: 0.6308 - val_loss: 1.7838 - val_acc: 0.4841\n",
      "Epoch 86/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 1.4683 - acc: 0.5683 - val_loss: 1.9540 - val_acc: 0.4385\n",
      "Epoch 87/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 1.9910 - acc: 0.4239 - val_loss: 1.9482 - val_acc: 0.4406\n",
      "Epoch 88/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.1603 - acc: 0.3914 - val_loss: 2.1842 - val_acc: 0.3758\n",
      "Epoch 89/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.2480 - acc: 0.3666 - val_loss: 2.0770 - val_acc: 0.4111\n",
      "Epoch 90/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.1349 - acc: 0.3890 - val_loss: 2.0308 - val_acc: 0.4245\n",
      "Epoch 91/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.1062 - acc: 0.3923 - val_loss: 2.0427 - val_acc: 0.4111\n",
      "Epoch 92/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.5592 - acc: 0.2958 - val_loss: 2.4777 - val_acc: 0.3114\n",
      "Epoch 93/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.6885 - acc: 0.2679 - val_loss: 2.3785 - val_acc: 0.3354\n",
      "Epoch 94/100\n",
      "26265/26265 [==============================] - 189s 7ms/sample - loss: 2.5924 - acc: 0.2865 - val_loss: 2.3641 - val_acc: 0.3306\n",
      "Epoch 95/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 2.5583 - acc: 0.2934 - val_loss: 2.3491 - val_acc: 0.3508\n",
      "Epoch 96/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.5375 - acc: 0.2980 - val_loss: 2.3249 - val_acc: 0.3518\n",
      "Epoch 97/100\n",
      "26265/26265 [==============================] - 187s 7ms/sample - loss: 2.4634 - acc: 0.3169 - val_loss: 2.2849 - val_acc: 0.3693\n",
      "Epoch 98/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 2.4134 - acc: 0.3235 - val_loss: 2.2516 - val_acc: 0.3755\n",
      "Epoch 99/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 2.3866 - acc: 0.3306 - val_loss: 2.2188 - val_acc: 0.3765\n",
      "Epoch 100/100\n",
      "26265/26265 [==============================] - 188s 7ms/sample - loss: 2.3426 - acc: 0.3419 - val_loss: 2.1866 - val_acc: 0.3827\n"
     ]
    }
   ],
   "source": [
    "units = 512  # From the mentioned article in data science\n",
    "epochs = 100  # Just a large number since I am using early stopping\n",
    "validationSplit = 0.1 # My data set is small so I want to use as much as possible to train vs. validate\n",
    "shuffle = False\n",
    "batchSize = 32 # Doubled the default batch size to speed up training\n",
    "dropOut = .5  # http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "    #Define callbacks\n",
    "\n",
    "mc = ModelCheckpoint('model.100Epoch_6.6.best', monitor='acc', mode='max', save_best_only=True) # Keep best model\n",
    "\n",
    "\n",
    "    # define and fit model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(dropOut))\n",
    "model.add(Dense(vocab_size, activation='softmax',kernel_constraint=maxnorm(3)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "modelLyrics = model.fit(X, y, epochs = epochs, validation_split = validationSplit, \n",
    "                            shuffle = shuffle, batch_size = batchSize, verbose=1,callbacks=[mc])\n",
    "\n",
    "   \n",
    "history = pd.DataFrame(modelLyrics.history)\n",
    "history.to_csv('modelLyricsHistory.100Epoch_6.6.2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "WARNING:tensorflow:From c:\\users\\administrator\\desktop\\projects\\nlp-country-song\\env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 2.9322 - acc: 0.2276 - val_loss: 2.5897 - val_acc: 0.3032\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.4171 - acc: 0.3365 - val_loss: 2.3031 - val_acc: 0.3582\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2339 - acc: 0.3676 - val_loss: 2.1955 - val_acc: 0.3913\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.1129 - acc: 0.4010 - val_loss: 2.1359 - val_acc: 0.3971\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.0526 - acc: 0.4241 - val_loss: 2.1109 - val_acc: 0.4191\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9236 - acc: 0.4550 - val_loss: 2.0108 - val_acc: 0.4513\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8273 - acc: 0.4849 - val_loss: 2.1109 - val_acc: 0.4486\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7193 - acc: 0.5059 - val_loss: 1.8773 - val_acc: 0.4665\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.5631 - acc: 0.5381 - val_loss: 1.8434 - val_acc: 0.4803\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.4330 - acc: 0.5733 - val_loss: 1.8295 - val_acc: 0.4839\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2985 - acc: 0.6101 - val_loss: 1.8329 - val_acc: 0.4884\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.1587 - acc: 0.6558 - val_loss: 1.8761 - val_acc: 0.4839\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.0136 - acc: 0.6941 - val_loss: 1.9417 - val_acc: 0.4861\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8742 - acc: 0.7400 - val_loss: 1.9659 - val_acc: 0.4888\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7420 - acc: 0.7793 - val_loss: 2.0353 - val_acc: 0.4924\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.6096 - acc: 0.8214 - val_loss: 2.1194 - val_acc: 0.4875\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4994 - acc: 0.8580 - val_loss: 2.2115 - val_acc: 0.4848\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4106 - acc: 0.8849 - val_loss: 2.2782 - val_acc: 0.4826\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3306 - acc: 0.9124 - val_loss: 2.4122 - val_acc: 0.4803\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2670 - acc: 0.9323 - val_loss: 2.4504 - val_acc: 0.4772\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2199 - acc: 0.9462 - val_loss: 2.5482 - val_acc: 0.4758\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1944 - acc: 0.9522 - val_loss: 2.5792 - val_acc: 0.4803\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1611 - acc: 0.9637 - val_loss: 2.6679 - val_acc: 0.4741\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1449 - acc: 0.9663 - val_loss: 2.7362 - val_acc: 0.4687\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.1297 - acc: 0.9700 - val_loss: 2.7390 - val_acc: 0.4808\n",
      "Epoch 00025: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 77s 4ms/sample - loss: 2.9814 - acc: 0.2135 - val_loss: 2.6643 - val_acc: 0.2809\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.4664 - acc: 0.3241 - val_loss: 2.3350 - val_acc: 0.3381\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2813 - acc: 0.3631 - val_loss: 2.2876 - val_acc: 0.3775\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2158 - acc: 0.3930 - val_loss: 2.0950 - val_acc: 0.3998\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 2.0184 - acc: 0.4207 - val_loss: 2.0094 - val_acc: 0.4231\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9052 - acc: 0.4485 - val_loss: 1.9575 - val_acc: 0.4410\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8105 - acc: 0.4731 - val_loss: 1.9057 - val_acc: 0.4548\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7126 - acc: 0.5006 - val_loss: 1.8791 - val_acc: 0.4674\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.6129 - acc: 0.5246 - val_loss: 1.8370 - val_acc: 0.4803\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.4978 - acc: 0.5554 - val_loss: 1.8186 - val_acc: 0.4946\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.3848 - acc: 0.5850 - val_loss: 1.8443 - val_acc: 0.4830\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2632 - acc: 0.6233 - val_loss: 1.8433 - val_acc: 0.4978\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.1315 - acc: 0.6630 - val_loss: 1.8736 - val_acc: 0.4857\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.9949 - acc: 0.7014 - val_loss: 1.9203 - val_acc: 0.4861\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8794 - acc: 0.7366 - val_loss: 1.9559 - val_acc: 0.4933\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.7557 - acc: 0.7700 - val_loss: 2.0227 - val_acc: 0.4919\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6528 - acc: 0.8025 - val_loss: 2.0961 - val_acc: 0.4794\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.5596 - acc: 0.8380 - val_loss: 2.1543 - val_acc: 0.4915\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.4713 - acc: 0.8621 - val_loss: 2.2746 - val_acc: 0.4799\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4034 - acc: 0.8815 - val_loss: 2.3406 - val_acc: 0.4835\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3462 - acc: 0.9028 - val_loss: 2.3916 - val_acc: 0.4879\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2946 - acc: 0.9181 - val_loss: 2.4468 - val_acc: 0.4870\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2622 - acc: 0.9286 - val_loss: 2.5483 - val_acc: 0.4665\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2275 - acc: 0.9390 - val_loss: 2.5753 - val_acc: 0.4794\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2092 - acc: 0.9429 - val_loss: 2.6601 - val_acc: 0.4741\n",
      "Epoch 26/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1834 - acc: 0.9504 - val_loss: 2.6506 - val_acc: 0.4812\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1805 - acc: 0.9495 - val_loss: 2.7751 - val_acc: 0.4785\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.1585 - acc: 0.9567 - val_loss: 2.7925 - val_acc: 0.4745\n",
      "Epoch 00028: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.9911 - acc: 0.2110 - val_loss: 2.6931 - val_acc: 0.2755\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.4789 - acc: 0.3250 - val_loss: 2.3549 - val_acc: 0.3430\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2811 - acc: 0.3593 - val_loss: 2.2496 - val_acc: 0.3855\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.2049 - acc: 0.3874 - val_loss: 2.1830 - val_acc: 0.4025\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 2.1813 - acc: 0.4099 - val_loss: 2.5470 - val_acc: 0.4056\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.9690 - acc: 0.4396 - val_loss: 1.9723 - val_acc: 0.4423\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.8334 - acc: 0.4675 - val_loss: 1.9113 - val_acc: 0.4499\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.7458 - acc: 0.4912 - val_loss: 1.8638 - val_acc: 0.4669\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.6482 - acc: 0.5175 - val_loss: 1.8499 - val_acc: 0.4727\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.5507 - acc: 0.5369 - val_loss: 1.8239 - val_acc: 0.4745\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.4430 - acc: 0.5698 - val_loss: 1.8174 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.3392 - acc: 0.5990 - val_loss: 1.8269 - val_acc: 0.4973\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.2091 - acc: 0.6371 - val_loss: 1.8889 - val_acc: 0.4785\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 1.0906 - acc: 0.6719 - val_loss: 1.8924 - val_acc: 0.4969\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.9833 - acc: 0.7006 - val_loss: 1.9166 - val_acc: 0.5004\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.8795 - acc: 0.7306 - val_loss: 1.9691 - val_acc: 0.4875\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.7814 - acc: 0.7619 - val_loss: 2.0401 - val_acc: 0.4897\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6836 - acc: 0.7903 - val_loss: 2.1004 - val_acc: 0.4799\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.6102 - acc: 0.8140 - val_loss: 2.1389 - val_acc: 0.4839\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 76s 4ms/sample - loss: 0.5286 - acc: 0.8416 - val_loss: 2.1998 - val_acc: 0.4870\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4718 - acc: 0.8601 - val_loss: 2.2694 - val_acc: 0.4946\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4066 - acc: 0.8790 - val_loss: 2.3399 - val_acc: 0.4924\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3597 - acc: 0.8926 - val_loss: 2.3755 - val_acc: 0.4839\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.3245 - acc: 0.9035 - val_loss: 2.4193 - val_acc: 0.4875\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2888 - acc: 0.9161 - val_loss: 2.5318 - val_acc: 0.4852\n",
      "Epoch 26/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2694 - acc: 0.9211 - val_loss: 2.5200 - val_acc: 0.4951\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2434 - acc: 0.9284 - val_loss: 2.6330 - val_acc: 0.4857\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2237 - acc: 0.9349 - val_loss: 2.6470 - val_acc: 0.4879\n",
      "Epoch 29/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2189 - acc: 0.9354 - val_loss: 2.6484 - val_acc: 0.4835\n",
      "Epoch 00029: early stopping\n",
      "Train on 20118 samples, validate on 2236 samples\n",
      "Epoch 1/100\n",
      "20118/20118 [==============================] - 77s 4ms/sample - loss: 2.9944 - acc: 0.2134 - val_loss: 2.6633 - val_acc: 0.2786\n",
      "Epoch 2/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.4989 - acc: 0.3190 - val_loss: 2.3626 - val_acc: 0.3453\n",
      "Epoch 3/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.3280 - acc: 0.3574 - val_loss: 2.3248 - val_acc: 0.3761\n",
      "Epoch 4/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.2743 - acc: 0.3847 - val_loss: 2.2233 - val_acc: 0.3998\n",
      "Epoch 5/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 2.1595 - acc: 0.4084 - val_loss: 2.0494 - val_acc: 0.4137\n",
      "Epoch 6/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.9781 - acc: 0.4251 - val_loss: 1.9959 - val_acc: 0.4141\n",
      "Epoch 7/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8943 - acc: 0.4497 - val_loss: 1.9433 - val_acc: 0.4419\n",
      "Epoch 8/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.8104 - acc: 0.4724 - val_loss: 1.9104 - val_acc: 0.4589\n",
      "Epoch 9/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.7279 - acc: 0.4900 - val_loss: 1.8599 - val_acc: 0.4794\n",
      "Epoch 10/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.6472 - acc: 0.5152 - val_loss: 1.8452 - val_acc: 0.4776\n",
      "Epoch 11/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.5553 - acc: 0.5420 - val_loss: 1.8437 - val_acc: 0.4732\n",
      "Epoch 12/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.4544 - acc: 0.5674 - val_loss: 1.8177 - val_acc: 0.4875\n",
      "Epoch 13/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.3615 - acc: 0.5945 - val_loss: 1.8391 - val_acc: 0.4830\n",
      "Epoch 14/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.2592 - acc: 0.6216 - val_loss: 1.8569 - val_acc: 0.4794\n",
      "Epoch 15/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.1541 - acc: 0.6543 - val_loss: 1.8636 - val_acc: 0.4839\n",
      "Epoch 16/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 1.0514 - acc: 0.6810 - val_loss: 1.8894 - val_acc: 0.4857\n",
      "Epoch 17/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.9467 - acc: 0.7134 - val_loss: 1.9431 - val_acc: 0.4790\n",
      "Epoch 18/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.8724 - acc: 0.7298 - val_loss: 1.9888 - val_acc: 0.4776\n",
      "Epoch 19/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7913 - acc: 0.7558 - val_loss: 2.0193 - val_acc: 0.4888\n",
      "Epoch 20/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.7093 - acc: 0.7831 - val_loss: 2.1060 - val_acc: 0.4843\n",
      "Epoch 21/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.6393 - acc: 0.8033 - val_loss: 2.1333 - val_acc: 0.4799\n",
      "Epoch 22/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.5724 - acc: 0.8235 - val_loss: 2.2034 - val_acc: 0.4817\n",
      "Epoch 23/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.5222 - acc: 0.8410 - val_loss: 2.2798 - val_acc: 0.4745\n",
      "Epoch 24/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4780 - acc: 0.8492 - val_loss: 2.3422 - val_acc: 0.4772\n",
      "Epoch 25/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.4222 - acc: 0.8710 - val_loss: 2.3686 - val_acc: 0.4776\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3969 - acc: 0.8775 - val_loss: 2.4419 - val_acc: 0.4799\n",
      "Epoch 27/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3617 - acc: 0.8875 - val_loss: 2.4361 - val_acc: 0.4821\n",
      "Epoch 28/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3303 - acc: 0.8973 - val_loss: 2.5147 - val_acc: 0.4741\n",
      "Epoch 29/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.3063 - acc: 0.9073 - val_loss: 2.5394 - val_acc: 0.4803\n",
      "Epoch 30/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2937 - acc: 0.9119 - val_loss: 2.5923 - val_acc: 0.4781\n",
      "Epoch 31/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2725 - acc: 0.9186 - val_loss: 2.6167 - val_acc: 0.4758\n",
      "Epoch 32/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2636 - acc: 0.9185 - val_loss: 2.6633 - val_acc: 0.4767\n",
      "Epoch 33/100\n",
      "20118/20118 [==============================] - 74s 4ms/sample - loss: 0.2376 - acc: 0.9290 - val_loss: 2.6864 - val_acc: 0.4830\n",
      "Epoch 34/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2300 - acc: 0.9281 - val_loss: 2.7175 - val_acc: 0.4691\n",
      "Epoch 35/100\n",
      "20118/20118 [==============================] - 75s 4ms/sample - loss: 0.2249 - acc: 0.9299 - val_loss: 2.7093 - val_acc: 0.4718\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Define parameters  -\n",
    "units = 512  # From the mentioned article in data science\n",
    "epochs = 100  # Just a large number since I am using early stopping\n",
    "validationSplit = 0.1 # My data set is small so I want to use as much as possible to train vs. validate\n",
    "shuffle = True\n",
    "batchSize = 64 # Doubled the default batch size to speed up training\n",
    "dropOut = .5  # http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "\n",
    "\n",
    "for i in range(2,6):\n",
    "    d = i*0.1\n",
    "    countryTrain(units,epochs,validationSplit,shuffle,batchSize,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that encodes a kickoff text string and then plugs it into our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_lyric, n_chars):\n",
    "    lyrics = seed_lyric\n",
    "    for __ in range(n_chars):\n",
    "    # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in lyrics]\n",
    "    # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "    # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "    # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "    # append to input\n",
    "        lyrics += char\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a song other than Stairway to Heaven that I could have used for the kickoff sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLyrics = \"I've had a largemouth bass bust my line A couple beautiful girls tell me, Goodbye Trucks break down,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLyrics = \"There's a lady who's sure All that glitters is gold And she's buying a stairway to heaven When she g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = open('lyrics.Coolio.Epoch100.txt','w')\n",
    "\n",
    "model = load_model('model.100Epoch_6.6.best')\n",
    "lyricsFinal = generate_seq(model,mapping,length,startLyrics,1000)\n",
    "__ = open('lyrics.Coolio.Epoch100.txt','a+')\n",
    "#__.write('Drop {}\\n\\n'.format(modelNum))\n",
    "__.write(lyricsFinal)\n",
    "__.write('.\\n\\n\\n')\n",
    "__.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and print the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(modelNum):\n",
    "    model = load_model('model.drop_0.{}.best'.format(modelNum))\n",
    "    lyricsFinal = generate_seq(model,mapping,length,startLyrics,1000)\n",
    "    __ = open('lyrics.LedZep.txt','a+')\n",
    "    __.write('Drop {}\\n\\n'.format(modelNum))\n",
    "    __.write(lyricsFinal)\n",
    "    __.write('.\\n\\n\\n')\n",
    "    __.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = open('lyrics.LedZep.txt','w')\n",
    "__.write('Stairway to Heaven/n/n')\n",
    "__.close()\n",
    "\n",
    "for i in range(0,6):\n",
    "    run_models(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_json('modelLyricsHistory.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-country-song",
   "language": "python",
   "name": "nlp-country-song"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
